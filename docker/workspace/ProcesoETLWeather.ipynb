{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "23/04/02 17:57:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\r\n",
    "from pyspark.sql.functions import to_json,col\r\n",
    "from pyspark.sql.types import *\r\n",
    "from os.path import abspath\r\n",
    "\r\n",
    "spark = SparkSession\\\r\n",
    "        .builder\\\r\n",
    "        .appName(\"pyspark-notebook\")\\\r\n",
    "        .master(\"spark://spark-master:7077\")\\\r\n",
    "        .config(\"spark.executor.memory\", \"512m\")\\\r\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\")\\\r\n",
    "        .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\")\\\r\n",
    "        .enableHiveSupport()\\\r\n",
    "        .getOrCreate()\r\n",
    "\r\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.3 MB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
      "Collecting numpy>=1.17.3\n",
      "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.7 MB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7.3->pandas) (1.12.0)\n",
      "Installing collected packages: numpy, pandas\n",
      "Successfully installed numpy-1.21.6 pandas-1.3.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.2; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting xlrd\n",
      "  Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: xlrd\n",
      "Successfully installed xlrd-2.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.2; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install xlrd\n",
    "\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aemet_data():\n",
    "    url = \"https://www.aemet.es/es/eltiempo/observacion/ultimosdatos_C447A_datos-horarios.xls?k=coo&l=C447A&datos=det&w=0&f=temperatura&x=\"\n",
    "    r = requests.get(url)\n",
    "    df = pd.read_excel(r.content, skiprows=3)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = get_aemet_data()\n",
    "datos = datos.rename(columns={\"Fecha y hora oficial\": 'Fecha', \"Temperatura (ºC)\" : 'Temperatura_c',\n",
    "                             \"Velocidad del viento (km/h)\": 'Velocidad_del_viento_km_h', \"Dirección del viento\": 'Direccion_del_viento',\n",
    "                             \"Racha (km/h)\": 'Racha_km_h', \"Dirección de racha\":'Direccion_de_racha',\n",
    "                             \"Precipitación (mm)\": 'Precipitacion_mm', \"Presión (hPa)\":'Presion_hPa',\n",
    "                             \"Tendencia (hPa)\": 'Tendencia_hPa', \"Humedad (%)\": 'Humedad_percent'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La fecha 02/04/2023 18:00 no tiene el formato correcto\n",
      "La fecha 02/04/2023 17:00 no tiene el formato correcto\n",
      "La fecha 02/04/2023 16:00 no tiene el formato correcto\n",
      "La fecha 02/04/2023 15:00 no tiene el formato correcto\n",
      "La fecha 02/04/2023 14:00 no tiene el formato correcto\n",
      "La fecha 02/04/2023 13:00 no tiene el formato correcto\n",
      "La fecha 02/04/2023 12:00 no tiene el formato correcto\n",
      "La fecha 02/04/2023 11:00 no tiene el formato correcto\n",
      "La fecha 02/04/2023 10:00 no tiene el formato correcto\n",
      "La fecha 02/04/2023 09:00 no tiene el formato correcto\n",
      "La fecha 02/04/2023 08:00 no tiene el formato correcto\n",
      "La fecha 02/04/2023 07:00 no tiene el formato correcto\n",
      "La fecha 02/04/2023 06:00 no tiene el formato correcto\n",
      "La fecha 02/04/2023 05:00 no tiene el formato correcto\n",
      "La fecha 02/04/2023 04:00 no tiene el formato correcto\n",
      "La fecha 02/04/2023 03:00 no tiene el formato correcto\n",
      "La fecha 02/04/2023 02:00 no tiene el formato correcto\n",
      "La fecha 02/04/2023 01:00 no tiene el formato correcto\n",
      "La fecha 02/04/2023 00:00 no tiene el formato correcto\n",
      "La fecha 01/04/2023 23:00 no tiene el formato correcto\n",
      "La fecha 01/04/2023 22:00 no tiene el formato correcto\n",
      "La fecha 01/04/2023 21:00 no tiene el formato correcto\n",
      "La fecha 01/04/2023 20:00 no tiene el formato correcto\n",
      "La fecha 01/04/2023 19:00 no tiene el formato correcto\n"
     ]
    }
   ],
   "source": [
    "for index, row in datos.iterrows():\n",
    "    fecha_no_f = row['Fecha']\n",
    "    try:\n",
    "        fecha_objeto = datetime.strptime(fecha_no_f, '%Y-%m-%d %H:%M:%S')\n",
    "        datos.iloc[index, datos.columns.get_loc('Fecha')] = fecha_objeto.strftime('%d/%m/%Y %H:%M')\n",
    "    except ValueError:\n",
    "        print(f'La fecha {fecha_no_f} no tiene el formato correcto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+-------------------------+--------------------+----------+------------------+----------------+-----------+-------------+---------------+\n",
      "|           Fecha|Temperatura_c|Velocidad_del_viento_km_h|Direccion_del_viento|Racha_km_h|Direccion_de_racha|Precipitacion_mm|Presion_hPa|Tendencia_hPa|Humedad_percent|\n",
      "+----------------+-------------+-------------------------+--------------------+----------+------------------+----------------+-----------+-------------+---------------+\n",
      "|02/04/2023 18:00|         22.4|                       12|                Este|        21|              Este|               0|      948.6|         -1.2|             47|\n",
      "|02/04/2023 17:00|         23.7|                       11|                Este|        21|           Sudeste|               0|      948.8|         -1.6|             37|\n",
      "|02/04/2023 16:00|         22.8|                       13|             Sudeste|        24|              Este|               0|      949.2|         -1.9|             44|\n",
      "+----------------+-------------+-------------------------+--------------------+----------+------------------+----------------+-----------+-------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField('Fecha', TimestampType(), True),\n",
    "    StructField('Temperatura_c', DoubleType(), True),\n",
    "    StructField('Velocidad_del_viento_km_h', DoubleType(), True),\n",
    "    StructField('Direccion_del_viento', StringType(), True),\n",
    "    StructField('Racha_km_h', DoubleType(), True),\n",
    "    StructField('Direccion_de_racha', StringType(), True),\n",
    "    StructField('Precipitacion_mm', DoubleType(), True),\n",
    "    StructField('Presion_hPa', DoubleType(), True),\n",
    "    StructField('Tendencia_hPa', DoubleType(), True),\n",
    "    StructField('Humedad_percent', DoubleType(), True)\n",
    "    \n",
    "])\n",
    "spark_df = spark.createDataFrame(datos)\n",
    "spark_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Carga de los datos a la tabla en hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_BASES = spark.sql(\"SHOW DATABASES \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          namespace|\n",
      "+-------------------+\n",
      "|            default|\n",
      "|        open_datadb|\n",
      "|open_weather_datadb|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_BASES.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use open_weather_datadb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df.write.mode('overwrite').saveAsTable('weather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = spark.sql(\"select * from weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+-------------------------+--------------------+----------+------------------+----------------+-----------+-------------+---------------+\n",
      "|           Fecha|Temperatura_c|Velocidad_del_viento_km_h|Direccion_del_viento|Racha_km_h|Direccion_de_racha|Precipitacion_mm|Presion_hPa|Tendencia_hPa|Humedad_percent|\n",
      "+----------------+-------------+-------------------------+--------------------+----------+------------------+----------------+-----------+-------------+---------------+\n",
      "|02/04/2023 06:00|         17.4|                       17|                 Sur|        30|           Sudeste|               0|      950.0|         -1.1|             40|\n",
      "|02/04/2023 05:00|         17.4|                       17|                 Sur|        30|           Sudeste|               0|      950.2|         -1.5|             36|\n",
      "|02/04/2023 04:00|         18.1|                       18|             Sudeste|        26|               Sur|               0|      950.6|         -1.7|             33|\n",
      "+----------------+-------------+-------------------------+--------------------+----------+------------------+----------------+-----------+-------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "a.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
